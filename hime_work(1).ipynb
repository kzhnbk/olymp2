{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea7d33-855e-4f8f-baa9-e2413098b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Directory path\n",
    "dir_input = '/kaggle/input/kazakhstan-ai-respa-take-home'\n",
    "train_df = pd.read_csv(f\"{dir_input}/train.csv\", parse_dates=['submitted_date'])\n",
    "test_df = pd.read_csv(f\"{dir_input}/test.csv\", parse_dates=['week_start','week_end'])\n",
    "submission_df = pd.read_csv(f\"{dir_input}/sample_submission.csv\")\n",
    "\n",
    "# Prepare weekly data\n",
    "train_df['week_start'] = train_df['submitted_date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weekly = (\n",
    "    train_df.groupby(['category','week_start'])['num_papers']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(['category','week_start'])\n",
    ")\n",
    "\n",
    "def create_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['t'] = np.arange(len(df))\n",
    "    df['month'] = df['week_start'].dt.month\n",
    "    df['quarter'] = df['week_start'].dt.quarter\n",
    "    df['sin_woy'] = np.sin(2 * np.pi * df['week_start'].dt.isocalendar().week / 52)\n",
    "    df['cos_woy'] = np.cos(2 * np.pi * df['week_start'].dt.isocalendar().week / 52)\n",
    "    df['year'] = df['week_start'].dt.year\n",
    "    df['day_of_year'] = df['week_start'].dt.dayofyear\n",
    "\n",
    "    lags = [1, 2, 3, 4, 8, 12, 26, 52]\n",
    "    windows = [4, 12, 26, 52]\n",
    "\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df['num_papers'].shift(lag)\n",
    "    for w in windows:\n",
    "        rolled = df['num_papers'].shift(1).rolling(w, min_periods=1)\n",
    "        df[f'roll_mean_{w}'] = rolled.mean()\n",
    "        df[f'roll_std_{w}'] = rolled.std()\n",
    "        df[f'roll_max_{w}'] = rolled.max()\n",
    "        df[f'roll_min_{w}'] = rolled.min()\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in df.columns:\n",
    "        if col.startswith('lag_'):\n",
    "            n = int(col.split('_')[1])\n",
    "            q = 0.005 if n >= 12 else 0.75\n",
    "            fill = df[col].quantile(q) if not df[col].isna().all() else 0\n",
    "            df[col] = df[col].fillna(fill)\n",
    "\n",
    "        elif col.startswith('roll_'):\n",
    "            w = int(col.split('_')[2])\n",
    "            q = 0.005 if w >= 12 else 0.75\n",
    "            fill = df[col].quantile(q) if not df[col].isna().all() else 0\n",
    "            df[col] = df[col].fillna(fill)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def create_lstm_dataset(X, y, timesteps=4):\n",
    "    \"\"\"Create dataset for LSTM model with specified timesteps\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - timesteps):\n",
    "        Xs.append(X[i:(i + timesteps)].values)\n",
    "        ys.append(y[i + timesteps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"Build and compile LSTM model\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=input_shape, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_prophet_model(df):\n",
    "    \"\"\"Train Prophet model on dataframe with columns 'week_start' and 'num_papers'\"\"\"\n",
    "    model_data = df[['week_start', 'num_papers']].rename(columns={'week_start': 'ds', 'num_papers': 'y'})\n",
    "    model = Prophet(yearly_seasonality=True, weekly_seasonality=True)\n",
    "    model.fit(model_data)\n",
    "    return model\n",
    "\n",
    "print('Starting enhanced model training...\\n')\n",
    "\n",
    "# Define expanded model dictionary\n",
    "test_models = {\n",
    "    'linear': Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(1, include_bias=False)), ('lr', LinearRegression())]),\n",
    "    'ridge1': Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(1, include_bias=False)), ('ridge', Ridge(alpha=1.0))]),\n",
    "    'poly2_lr': Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(2, include_bias=False)), ('lr', LinearRegression())]),\n",
    "    'ridge2': Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(2, include_bias=False)), ('ridge', Ridge(alpha=1.0))]),\n",
    "    'xgboost': xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'lightgbm': lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'catboost': cb.CatBoostRegressor(\n",
    "        iterations=100,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        loss_function='RMSE',\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    ),\n",
    "    'random_forest': RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        min_samples_split=5,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "min_scale = 10\n",
    "val_weeks = 8\n",
    "results = []\n",
    "preds = []\n",
    "all_smape = []\n",
    "best_models = {}\n",
    "\n",
    "for cat, group in weekly.groupby('category'):\n",
    "    print(f\"\\nProcessing category: {cat}\")\n",
    "    df_feat = create_time_features(group)\n",
    "    X = df_feat.drop(['category', 'week_start', 'num_papers'], axis=1)\n",
    "    y = df_feat['num_papers']\n",
    "    \n",
    "    # Special handling for short series\n",
    "    if len(df_feat) <= val_weeks + 4:  # Need at least 4 points after validation for LSTM\n",
    "        print(f\"  Series too short ({len(df_feat)} points), using default model\")\n",
    "        best_model = test_models['poly2_lr']\n",
    "        best_name = 'poly2_lr'\n",
    "    else:\n",
    "        # Standard validation\n",
    "        df_tr = df_feat.iloc[:-val_weeks]\n",
    "        df_val = df_feat.iloc[-val_weeks:]\n",
    "        X_tr = df_tr.drop(['category', 'week_start', 'num_papers'], axis=1)\n",
    "        y_tr = df_tr['num_papers']\n",
    "        X_val = df_val.drop(['category', 'week_start', 'num_papers'], axis=1)\n",
    "        y_val = df_val['num_papers']\n",
    "        \n",
    "        # Evaluate standard models\n",
    "        smape_scores = {}\n",
    "        for name, model in test_models.items():\n",
    "            print(f\"  Training {name}...\")\n",
    "            m = model\n",
    "            m.fit(X_tr, y_tr)\n",
    "            y_pred = m.predict(X_val)\n",
    "            smape = np.mean(np.abs(y_pred - y_val) / np.maximum(np.abs(y_val), min_scale))\n",
    "            smape_scores[name] = smape\n",
    "            results.append({'category': cat, 'model': name, 'smape': smape})\n",
    "            \n",
    "        # Try LSTM if we have enough data\n",
    "        if len(df_tr) >= 8:  # Need at least 8 points for a meaningful LSTM\n",
    "            print(f\"  Training LSTM...\")\n",
    "            try:\n",
    "                # Prepare LSTM data\n",
    "                X_lstm_cols = X_tr.columns\n",
    "                scaler_X = StandardScaler()\n",
    "                scaler_y = StandardScaler()\n",
    "                \n",
    "                X_tr_scaled = pd.DataFrame(scaler_X.fit_transform(X_tr), columns=X_lstm_cols)\n",
    "                y_tr_scaled = scaler_y.fit_transform(y_tr.values.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                X_val_scaled = pd.DataFrame(scaler_X.transform(X_val), columns=X_lstm_cols)\n",
    "                \n",
    "                # Create sequences for LSTM\n",
    "                timesteps = min(4, len(X_tr) // 2)  # Adjust timesteps based on data size\n",
    "                X_lstm_tr, y_lstm_tr = create_lstm_dataset(X_tr_scaled, y_tr_scaled, timesteps)\n",
    "                \n",
    "                if len(X_lstm_tr) > 0:\n",
    "                    # Build and train LSTM\n",
    "                    input_shape = (X_lstm_tr.shape[1], X_lstm_tr.shape[2])\n",
    "                    lstm_model = build_lstm_model(input_shape)\n",
    "                    early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "                    lstm_model.fit(\n",
    "                        X_lstm_tr, y_lstm_tr,\n",
    "                        epochs=50,\n",
    "                        batch_size=min(8, len(X_lstm_tr)),\n",
    "                        callbacks=[early_stop],\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    lstm_preds = []\n",
    "                    last_sequence = X_tr_scaled.values[-timesteps:].reshape(1, timesteps, -1)\n",
    "                    \n",
    "                    for _ in range(len(X_val)):\n",
    "                        pred = lstm_model.predict(last_sequence, verbose=0)[0][0]\n",
    "                        lstm_preds.append(pred)\n",
    "                        \n",
    "                        # Update sequence for next prediction\n",
    "                        next_input = X_val_scaled.iloc[len(lstm_preds)-1:len(lstm_preds)].values\n",
    "                        last_sequence = np.append(last_sequence[:, 1:, :], [next_input], axis=1)\n",
    "                    \n",
    "                    # Inverse transform predictions\n",
    "                    lstm_preds = scaler_y.inverse_transform(np.array(lstm_preds).reshape(-1, 1)).flatten()\n",
    "                    \n",
    "                    # Calculate SMAPE\n",
    "                    lstm_smape = np.mean(np.abs(lstm_preds - y_val) / np.maximum(np.abs(y_val), min_scale))\n",
    "                    smape_scores['lstm'] = lstm_smape\n",
    "                    results.append({'category': cat, 'model': 'lstm', 'smape': lstm_smape})\n",
    "            except Exception as e:\n",
    "                print(f\"  LSTM error: {str(e)}\")\n",
    "                \n",
    "        # Try Prophet if we have enough data\n",
    "        if len(df_tr) >= 10:  # Prophet needs a decent amount of data\n",
    "            print(f\"  Training Prophet...\")\n",
    "            try:\n",
    "                prophet_model = train_prophet_model(df_tr[['week_start', 'num_papers']])\n",
    "                \n",
    "                # Make predictions\n",
    "                future = prophet_model.make_future_dataframe(periods=val_weeks, freq='W')\n",
    "                forecast = prophet_model.predict(future)\n",
    "                prophet_preds = forecast['yhat'].iloc[-val_weeks:].values\n",
    "                \n",
    "                # Calculate SMAPE\n",
    "                prophet_smape = np.mean(np.abs(prophet_preds - y_val) / np.maximum(np.abs(y_val), min_scale))\n",
    "                smape_scores['prophet'] = prophet_smape\n",
    "                results.append({'category': cat, 'model': 'prophet', 'smape': prophet_smape})\n",
    "            except Exception as e:\n",
    "                print(f\"  Prophet error: {str(e)}\")\n",
    "                \n",
    "        # Try ARIMA if we have enough data\n",
    "        if len(df_tr) >= 12:  # ARIMA needs a decent amount of data\n",
    "            print(f\"  Training ARIMA...\")\n",
    "            try:\n",
    "                arima_model = ARIMA(df_tr['num_papers'], order=(1, 1, 1))\n",
    "                arima_res = arima_model.fit()\n",
    "                \n",
    "                # Make predictions\n",
    "                arima_preds = arima_res.forecast(steps=val_weeks)\n",
    "                \n",
    "                # Calculate SMAPE\n",
    "                arima_smape = np.mean(np.abs(arima_preds - y_val) / np.maximum(np.abs(y_val), min_scale))\n",
    "                smape_scores['arima'] = arima_smape\n",
    "                results.append({'category': cat, 'model': 'arima', 'smape': arima_smape})\n",
    "            except Exception as e:\n",
    "                print(f\"  ARIMA error: {str(e)}\")\n",
    "                \n",
    "        # Select best model\n",
    "        best_name = min(smape_scores, key=smape_scores.get)\n",
    "        print(f\"  Best model: {best_name} with sMAPE={smape_scores[best_name]:.4f}\")\n",
    "        all_smape.append(smape_scores[best_name])\n",
    "        \n",
    "        # For specialized models, we'll need to store them differently\n",
    "        if best_name == 'lstm':\n",
    "            best_models[cat] = {\n",
    "                'type': 'lstm',\n",
    "                'model': lstm_model,\n",
    "                'scaler_X': scaler_X,\n",
    "                'scaler_y': scaler_y,\n",
    "                'timesteps': timesteps,\n",
    "                'X_cols': X_lstm_cols\n",
    "            }\n",
    "            continue\n",
    "        elif best_name == 'prophet':\n",
    "            best_models[cat] = {\n",
    "                'type': 'prophet',\n",
    "                'model': prophet_model\n",
    "            }\n",
    "            continue\n",
    "        elif best_name == 'arima':\n",
    "            best_models[cat] = {\n",
    "                'type': 'arima',\n",
    "                'model': arima_res\n",
    "            }\n",
    "            continue\n",
    "        else:\n",
    "            best_model = test_models[best_name]\n",
    "            best_models[cat] = {\n",
    "                'type': 'standard',\n",
    "                'model': best_model,\n",
    "                'name': best_name\n",
    "            }\n",
    "    \n",
    "    # For standard models, fit on all data\n",
    "    if cat not in best_models:\n",
    "        best_model = test_models[best_name]\n",
    "        best_model.fit(X, y)\n",
    "        best_models[cat] = {\n",
    "            'type': 'standard',\n",
    "            'model': best_model,\n",
    "            'name': best_name\n",
    "        }\n",
    "\n",
    "# Make predictions on test data\n",
    "for cat, cat_test in test_df.groupby('category'):\n",
    "    print(f\"\\nGenerating predictions for {cat}...\")\n",
    "    cat_test = cat_test.sort_values('week_id').copy()\n",
    "    hist = weekly[weekly['category'] == cat].set_index('week_start')['num_papers'].copy()\n",
    "    test_preds = []\n",
    "    \n",
    "    model_info = best_models.get(cat)\n",
    "    if model_info is None:\n",
    "        print(f\"  No model found for {cat}, using default\")\n",
    "        model_info = {\n",
    "            'type': 'standard',\n",
    "            'model': test_models['ridge2'],\n",
    "            'name': 'ridge2'\n",
    "        }\n",
    "    \n",
    "    model_type = model_info['type']\n",
    "    \n",
    "    # Different prediction process depending on model type\n",
    "    if model_type == 'standard':\n",
    "        print(f\"  Using standard model: {model_info.get('name', 'unknown')}\")\n",
    "        for _, row in cat_test.iterrows():\n",
    "            wk = row['week_start']\n",
    "            hist = pd.concat([hist, pd.Series({wk: np.nan})])\n",
    "            temp = pd.DataFrame({\n",
    "                'category': cat,\n",
    "                'week_start': hist.index,\n",
    "                'num_papers': hist.values\n",
    "            })\n",
    "            feat = create_time_features(temp)\n",
    "            X_test = feat.drop(['category', 'week_start', 'num_papers'], axis=1).iloc[[-1]]\n",
    "            y_hat = model_info['model'].predict(X_test)[0]\n",
    "            y_hat = max(0, y_hat)\n",
    "            test_preds.append(y_hat)\n",
    "            hist.iloc[-1] = y_hat\n",
    "            \n",
    "    elif model_type == 'lstm':\n",
    "        print(f\"  Using LSTM model\")\n",
    "        # For LSTM we need a different approach since it uses sequences\n",
    "        lstm_model = model_info['model']\n",
    "        scaler_X = model_info['scaler_X']\n",
    "        scaler_y = model_info['scaler_y']\n",
    "        timesteps = model_info['timesteps']\n",
    "        X_cols = model_info['X_cols']\n",
    "        \n",
    "        # Get the historical data with features\n",
    "        hist_df = weekly[weekly['category'] == cat].copy()\n",
    "        hist_feat = create_time_features(hist_df)\n",
    "        X_hist = hist_feat.drop(['category', 'week_start', 'num_papers'], axis=1)\n",
    "        \n",
    "        # Scale the data\n",
    "        X_hist_scaled = pd.DataFrame(scaler_X.transform(X_hist), columns=X_cols)\n",
    "        \n",
    "        # Create initial sequence\n",
    "        last_sequence = X_hist_scaled.values[-timesteps:].reshape(1, timesteps, -1)\n",
    "        \n",
    "        for _, row in cat_test.iterrows():\n",
    "            wk = row['week_start']\n",
    "            \n",
    "            # Predict next value\n",
    "            pred_scaled = lstm_model.predict(last_sequence, verbose=0)[0][0]\n",
    "            y_hat = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
    "            y_hat = max(0, y_hat)\n",
    "            test_preds.append(y_hat)\n",
    "            \n",
    "            # Update history\n",
    "            hist = pd.concat([hist, pd.Series({wk: y_hat})])\n",
    "            \n",
    "            # Create new features for this point\n",
    "            temp = pd.DataFrame({\n",
    "                'category': cat,\n",
    "                'week_start': hist.index,\n",
    "                'num_papers': hist.values\n",
    "            })\n",
    "            new_feat = create_time_features(temp)\n",
    "            new_X = new_feat.drop(['category', 'week_start', 'num_papers'], axis=1).iloc[[-1]]\n",
    "            \n",
    "            # Scale new features\n",
    "            new_X_scaled = scaler_X.transform(new_X)\n",
    "            \n",
    "            # Update sequence for next prediction\n",
    "            last_sequence = np.append(last_sequence[:, 1:, :], [new_X_scaled], axis=1)\n",
    "            \n",
    "    elif model_type == 'prophet':\n",
    "        print(f\"  Using Prophet model\")\n",
    "        prophet_model = model_info['model']\n",
    "        \n",
    "        # Create future dataframe with test weeks\n",
    "        future_dates = pd.concat([\n",
    "            pd.DataFrame({'ds': weekly[weekly['category'] == cat]['week_start']}),\n",
    "            pd.DataFrame({'ds': cat_test['week_start']})\n",
    "        ]).drop_duplicates().sort_values('ds')\n",
    "        \n",
    "        forecast = prophet_model.predict(future_dates)\n",
    "        \n",
    "        # Extract predictions for test weeks\n",
    "        for _, row in cat_test.iterrows():\n",
    "            wk = row['week_start']\n",
    "            y_hat = forecast[forecast['ds'] == wk]['yhat'].values[0]\n",
    "            y_hat = max(0, y_hat)\n",
    "            test_preds.append(y_hat)\n",
    "            hist = pd.concat([hist, pd.Series({wk: y_hat})])\n",
    "            \n",
    "    elif model_type == 'arima':\n",
    "        print(f\"  Using ARIMA model\")\n",
    "        arima_res = model_info['model']\n",
    "        \n",
    "        # For ARIMA, we need to forecast one step at a time and update\n",
    "        for _, row in cat_test.iterrows():\n",
    "            wk = row['week_start']\n",
    "            y_hat = arima_res.forecast(steps=1)[0]\n",
    "            y_hat = max(0, y_hat)\n",
    "            test_preds.append(y_hat)\n",
    "            \n",
    "            # Update history and refit model\n",
    "            hist = pd.concat([hist, pd.Series({wk: y_hat})])\n",
    "            try:\n",
    "                # Update ARIMA model with new observation\n",
    "                arima_model = ARIMA(hist, order=(1, 1, 1))\n",
    "                arima_res = arima_model.fit()\n",
    "            except:\n",
    "                # If refitting fails, keep using the last model\n",
    "                pass\n",
    "    \n",
    "    cat_test['num_papers'] = test_preds\n",
    "    preds.append(cat_test[['category', 'week_id', 'num_papers']])\n",
    "\n",
    "# Combine predictions and create submission\n",
    "if all_smape:\n",
    "    print(f\"\\nOverall average sMAPE: {np.mean(all_smape):.4f}\")\n",
    "\n",
    "pred_df = pd.concat(preds)\n",
    "pred_df['id'] = pred_df['category'] + '__' + pred_df['week_id'].astype(str)\n",
    "submission = (\n",
    "    submission_df[['id']]\n",
    "    .merge(pred_df[['id', 'num_papers']], on='id', how='left')\n",
    ")\n",
    "submission['num_papers'] = submission['num_papers'].fillna(0).round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\nSubmission file created: submission.csv\")\n",
    "\n",
    "# Analysis of model performance\n",
    "model_results = pd.DataFrame(results)\n",
    "if not model_results.empty:\n",
    "    print(\"\\nModel performance summary:\")\n",
    "    model_summary = model_results.groupby('model')['smape'].agg(['mean', 'std', 'count']).sort_values('mean')\n",
    "    print(model_summary)\n",
    "\n",
    "    print(\"\\nTop model by category:\")\n",
    "    top_models = model_results.loc[model_results.groupby('category')['smape'].idxmin()]\n",
    "    print(top_models[['category', 'model', 'smape']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
